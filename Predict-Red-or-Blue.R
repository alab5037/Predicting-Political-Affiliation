rm(list=ls())

###1.Preprocessing Data###

##import data##
library('readxl')
library('tidyr')
customer=read_excel('/Users/halabanz/Desktop/Applied Multivariate/customers.xlsx')
gen=read_excel('/Users/halabanz/Desktop/Applied Multivariate/generation.xlsx')
redvsblue=read_excel('/Users/halabanz/Desktop/Applied Multivariate/redvsblue.xlsx')
re=read_excel('/Users/halabanz/Desktop/Applied Multivariate/re.xlsx')

##clean customer data (2016)##
str(customer) #look at structure of dataset
customer2=customer[customer$Year==2017 & customer$`Industry Sector Category`=='Total Electric Industry',-c(3:8)] #select row for: 2016 entries and Total Electric Industry
sort(customer$State) #sort by alphabetical order, asc

##clean generation data (2016)##
str(gen)
gen2=gen[gen$YEAR==2017 & gen$`TYPE OF PRODUCER`=='Total Electric Power Industry',-c(3)] 
gen3=spread(gen2,`ENERGY SOURCE`,`GENERATION (Megawatthours)`) #reshape from long to wide with energy source as one column and gen in other
colnames(gen3)[5]='Hydroelectric'
sort(gen3$STATE)
summary(gen3)
gen4=gen3[,-c(4,10,12,14)] #remove columns not used in analysis

##merge data##
m1=merge(gen4,customer2,by.x = 'STATE',by.y = 'State') #merge generation and customer data
m2=m1[,-c(13)] #get rid of duplicate year
colnames(m2)[13]='Total_Customers' #rename column
m3=merge(m2,redvsblue,by.x='STATE',by.y = 'STATE') #merge m2 and data for political affilation
summary(m3)

##impute missing values to 0##
m3[is.na(m3)] <- 0 #after doing some research, data seems to be missing because generation was at 0 or close to it
str(m3)
summary(m3)

##transform variables (calculate total generation per customer)##
m4=m3 #making sure that data (m3) is available in its original form

#first create function for total gen/cust
gen_per_cust=function(col) {
  col/m4$Total_Customers
}

#create new columns for generation per customer (all sources)
for(i in 3:12) {
  m4[[paste(substr(colnames(m4)[[i]],1,7),'per_customer',sep="_")]] = gen_per_cust(m4[[i]])
}

m4=m4[,-c(3:13)] #drop old variables
colnames(m4)[c(4:13)]=c('coal','hydro','ng','nuclear','other','bio','petrol','solar','wind','wood') #change names

m4$Affiliation=as.factor(m4$Affiliation) #change affiliation from 2 to 4 to 0 to 1
levels(m4$Affiliation)=c('Red State','Blue State')
contrasts(m4$Affiliation)

str(m4) #check structure of final dataframe
m4[,4:13] <- lapply(m4[,4:13], as.numeric) #change from character to numeric


###2.Exploratory Analysis###

##2.1 bar charts of electricity generation by source##
library('ggplot2')
library('dplyr')
library('scales')

#calculate electricity generated by percentage of total generation#
m5=m3 #making sure that data (m3) is available in its original form

#first create function for total gen/cust
elec_perc=function(col) {
  round((sum(col) / sum(m5[,c(3:12)]))*100,1)
}

percent_by_source=apply(m5[,c(3:12)], 2,elec_perc) #store percentages
percent_by_source=data.frame(percent_by_source) #convert to df
library(data.table)
setDT(percent_by_source, keep.rownames = TRUE)[] #convert index to column
str(percent_by_source)

plot1=ggplot(data=percent_by_source,aes(x=rn,y=percent_by_source,fill=rn))+geom_bar(stat='identity')+xlab('Source')+
  ylab('% Total')+ggtitle('Electricity Generation in United States (2016)')+ylim(0,50)+
  theme(plot.title = element_text(hjust = 0.5))+theme(axis.text.x = element_blank());plot1


##2.2 generate usa heatmap of electricity generation (Alaska and Hawaii included)##
install.packages("devtools")
devtools::install_github("wmurphyrd/fiftystater")
library(fiftystater)

data("fifty_states") # this line is optional due to lazy data loading
renewables <- data.frame(state = tolower(rownames(re)), re) #plug in re dataset here

#map_id creates the aesthetic mapping to the state name column in your data#
p <- ggplot(renewables, aes(map_id = State)) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = PercentRenewables), map = fifty_states) + 
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() +
  scale_x_continuous(breaks = NULL) + 
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom", 
        panel.background = element_blank());p


##2.3 boxplots##
m6=m4[,c(4:13)]
m6$other[m6$other<0]=0 #need get rid of negative values, since sqrt variables 
                       #only 3 negative values and they are small, so shouldnt impact visualization
m6[] <- lapply(m6,sqrt) #sqrt values in dataframe in order to make differences more clear in plot
m6=cbind(m4[,c(1,2,3)],m6) #append state, year, and affiliation to dataframe

aggregate(m6[,4:13],list(m6$Affiliation),mean) #numbers indicate large mean differences in coal, bio, and petrol
long_DF <- m6 %>% gather(Source, Generation, coal:wood) #reshape from wide to long in order to plot
plot2=ggplot(data=long_DF,aes(x=Source,y=Generation,fill=Affiliation))+geom_boxplot()+xlab('Electricity Generation by Source')+ylab('Generation per Customer (âˆšMWH)')+ggtitle('Red States vs. Blue States (2016)')+  theme(plot.title = element_text(hjust = 0.5));plot2

##view correlation between variables##
m4.cor = cor(m4[,c(4:13)]);m4.cor


###3.Building Models###

##3.1 Create training and test sets##
Train <- createDataPartition(m4$Affiliation, p=0.80, list=FALSE) 
m4_train <- m4[Train, ]
m4_test <- m4[ -Train, ]

##3.2 logistic regression (repeated 3 times to assess stability)##
install.packages("stargazer")
library('stargazer')
library('MASS')
library('car')
library('caret')

##3.1 Create training and test sets##
Train <- createDataPartition(m4$Affiliation, p=0.80, list=FALSE) 
m4_train <- m4[Train, ]
m4_test <- m4[-Train, ]

#first run full model#
logit1=glm(Affiliation ~ coal+wind+solar+bio+hydro+petrol+nuclear+wood+ng+other,family=binomial(link='logit'),data=m4_train);logit1
summary(logit1) #only coal and petrol statistically significant
logit1$fitted.values
vif1=vif(logit1);vif1 #all vif lower than 10, so no real issue with multicollinearity

#variable selection 1#
drop1(logit1,test = 'LRT') #keep coal bio ng
anova(logit1,test='LRT') #keep coal bio ng
drop1(logit1,test = 'Rao') #keep bio petrol
anova(logit1,test='Rao') #keep bio coal

#update model 1#
logit2=glm(Affiliation ~ coal+bio+ng+petrol,family=binomial(link='logit'),data = m4_train)
summary(logit2) #coal bio petrol statistically significant

#variable selection 2#
drop1(logit2,test = 'LRT') #keep coal bio 
anova(logit2,test='LRT') #keep coal bio 
drop1(logit2,test = 'Rao') #bio petrol
anova(logit2,test='Rao') #coal bio petrol

#update model 2, drop ng#
logit3=glm(Affiliation ~ coal+bio+petrol,family=binomial(link='logit'),data = m4_train)
summary(logit3) 

#variable selection 3#
drop1(logit3,test = 'LRT') #keep variables coal and bio
anova(logit3,test='LRT') #keep variables coal and bio
drop1(logit3,test = 'Rao') #bio petrol
anova(logit3,test='Rao') #keep all

#since no variables can be decisively dropped, logit3 model is final (results similar all 3 times)#
summary(logit3) #bio always significant at 5% level
                #petrol is usually significant at either 5% or 10% level (less stable)
                #coal is sometimes significant at the 10% level (less stable)

#prediction (average on all 3 training sets)#
p1 <- predict(logit3,newdata=m4_train,type='response')
p1 <- ifelse(p1 > 0.5,1,0)
p1=as.factor(p1)
str(p1)
levels(m4_train$Affiliation) <- c("0", "1")
misClasificError <- mean(p1 != m4_train$Affiliation);misClasificError
confusion <- table(p1,m4_train$Affiliation);confusion #in all 3 cases, false pos. > false neg.
print(paste('Accuracy',1-misClasificError)) #90% average accuracy (1: 90%, 2: 88%, 3: 92%)

#prediction (average on all 3 test sets)#
p2 <- predict(logit3,newdata=m4_test,type='response')
p2 <- ifelse(p2 > 0.5,1,0)
p2=as.factor(p2)
levels(m4_test$Affiliation) <- c("0", "1")
misClasificError <- mean(p2 != m4_test$Affiliation);misClasificError
confusion <- table(p2,m4_test$Affiliation);confusion #in all 3 cases, false pos. > false neg.
print(paste('Accuracy',1-misClasificError)) #84% average accuracy (1: 88%, 2: 88%, 3: 77%))

#k-folds cross validation#
ctrl <- trainControl(method = "repeatedcv", number = 5, savePredictions = TRUE)
mod_fit <- train(Affiliation ~ coal+bio+petrol,  
                 data=m4, method="glm", family="binomial",
                 trControl = ctrl, tuneLength = 5)
summary(mod_fit) #output similar to previous 
p3 = predict(mod_fit, newdata=m4_test,type = 'prob');p3
p3 <- ifelse(p3$`Blue State` > 0.5,1,0)
p3=as.factor(p3);p3
levels(m4_test$Affiliation) <- c("0", "1")
misClasificError <- mean(p3 != m4_test$Affiliation);misClasificError
confusion <- table(p3,m4_test$Affiliation);confusion 
print(paste('Accuracy',1-misClasificError)) #88% accuracy

#Analyze quality of model#
install.packages('generalhoslem') #Hosmer-Lemeshow Stat
library('generalhoslem')
generalhoslem::logitgof(m4_train$Affiliation,fitted(logit3)) #p-value of 0.81 shows no evidence that predicted different from observed

install.packages('fmsb') #Neagelkerke
library('fmsb')
NagelkerkeR2(logit3) #R2 value of 0.72 indicates a model with decent prediction

OptimisedConc=function(model) #Concordance measure
{
  Data = cbind(model$y, model$fitted.values) 
  ones = Data[Data[,1] == 1,]
  zeros = Data[Data[,1] == 0,]
  conc=matrix(0, dim(zeros)[1], dim(ones)[1])
  disc=matrix(0, dim(zeros)[1], dim(ones)[1])
  ties=matrix(0, dim(zeros)[1], dim(ones)[1])
  for (j in 1:dim(zeros)[1])
  {
    for (i in 1:dim(ones)[1])
    {
      if (ones[i,2]>zeros[j,2])
      {conc[j,i]=1}
      else if (ones[i,2]<zeros[j,2])
      {disc[j,i]=1}
      else if (ones[i,2]==zeros[j,2])
      {ties[j,i]=1}
    }
  }
  Pairs=dim(zeros)[1]*dim(ones)[1]
  PercentConcordance=(sum(conc)/Pairs)*100
  PercentDiscordance=(sum(disc)/Pairs)*100
  PercentTied=(sum(ties)/Pairs)*100
  return(list("Percent Concordance"=PercentConcordance,"Percent Discordance"=PercentDiscordance,"Percent Tied"=PercentTied,"Pairs"=Pairs))
}
OptimisedConc(logit3) #model shows prediction of 95%, which is very good

#check for outliers#
N=length(m4_train$STATE) #global influence plot
STATE=1:N
hat.bw=hatvalues(logit3)
rstudent.bw=rstudent(logit3)
par(mfrow=c(2,2))
plot(hat.bw,rstudent.bw)
dffits.bw=dffits(logit3)
plot(STATE,dffits.bw,type = 'l')
cov.bw=covratio(logit3)
plot(STATE,cov.bw,type = 'l')
cook.bw=cooks.distance(logit3)
plot(STATE,cook.bw,type = 'l')
par(mfrow=c(1,1))
plot(STATE,cook.bw)
identify(STATE,cook.bw)
cook.bw

#observations 12 (HI), 14 (ID),and 33 (NM) look to be potential influential obs.
#Hawaii produces a lot of petrol for a blue state
#NM is a blue state that doesnt produce much biomass (37th) 
#ID produces the least amount of coal (21st) and most biomass (47th) per customer out of any red state

#create boxplots to compare outliers of ID and NM#
#coal for outliers compared to nations average#
plot3=ggplot(data = m6[33,],aes(x=STATE,y=coal))+geom_bar(stat='identity',fill='blue',width = 0.5);plot3
aggregate(m6$coal,list(m6$Affiliation),mean)
plot4=plot3+geom_bar(data=m6[14,],aes(x=STATE,y=coal),stat = 'identity',fill='red',width = 0.5)+geom_hline(data=m6,yintercept =  c(1.71),list(m6),color='blue')+xlab('States')+ylab('Coal per Customer (âˆšMWH)')+ggtitle('Comparison to Affiliaton Average: Coal (2016)')+theme(plot.title = element_text(hjust = 0.5));plot4
plot5=plot4+geom_hline(data=m6,yintercept =  c(3.981),list(m6),color='red');plot5

#biomass in idaho compared to nations average#
plot6=ggplot(data=m6[14,],aes(x=STATE,y=bio))+geom_bar(stat='identity',fill='red',width = 0.5);plot6
#bio for co and nm#
plot7=plot6+geom_bar(data=m6[33,],aes(x=STATE,y=bio),stat='identity',fill='blue',width = 0.5);plot7
#bio for outliers compared to nations average#
aggregate(m6$bio,list(m7$Affiliation),mean)
plot8=plot7+geom_hline(data=m6,yintercept = 0.19,color='red')+xlab('States')+ylab('Bio per Customer (âˆšMWH)')+ggtitle('Comparison to Affiliation Averages: Bio (2016)')+theme(plot.title = element_text(hjust = 0.5));plot7
plot9=plot8+geom_hline(data=m6,yintercept = 0.4286,color='blue');plot9

#new dataset with percentile ranks#
m7=m4[,c(1,3,5,8)]
percentile_df=mutate(m7,percent_rank_coal=ntile(m7$coal,100))
percentile_df=mutate(percentile_df,percent_rank_bio=ntile(percentile_df$bio,100))

#create tables for latex#
stargazer(logit3,title="Results", align=F)
model=factor(c('Reduced Model (bio+coal'))
misclassification_rate=c(misClasificError)
combine=data.frame(model,misclassification_rate)
library('xtable')
xtable(combine)


##3.3 random forest##
library('randomForest')
library('caret')

#build model#
par(mfrow=c(1,1))
rf1=randomForest(m4[,4:13],m4$Affiliation,mtry=3,data=m4,importance = TRUE,proximity = TRUE,ntree = 200) #first use default values for ntree and mtry
print(rf1) #oob error 19.61%, so 80% accuracy

#error rate of random forest
plot(rf1) #error stabilizes after 100 trees

#tune mtry
par(mfrow=c(1,1))
tuneRF(m4[,-c(1)],m4[,c(1)],stepFactor = 0.5,plot = TRUE,ntreeTry = 100,trace = TRUE,improve = 0.4) #anything above 3 mtry is the same meaning that all variables
varImpPlot(rf1) #bio and coal look to be only important variables


##3.4. KNN##
library(class)

##Generate a random number that is 80% of the total number of rows in dataset##
gen_rand <- sample(1:nrow(m6), 0.80 * nrow(m4)) 

##create normalization function##
normalization <-function(x) { (x -min(x))/(max(x)-min(x))} 
m4_norm <- as.data.frame(lapply(m4[,c(4:13)], normalization)) #run nomalization on all features
summary(m4_norm)

##create training/test##
m4_train <- m4_norm[gen_rand,] #create training
m4_test <- m4_norm[-gen_rand,] #create test 
m4_target_cat <- m4[gen_rand,3] #3th column is reponse, political affiliation
m4_test_cat <- m4[-gen_rand,3] #actual values

##run model (done 3 times to assess stability)##
knn1 <- knn(m4_train,m4_test,cl=m4_target_cat,k=3)  
confusion <- table(knn1,m4_test_cat) #confusion matrix
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(confusion) #81% average accuracy (1: 81%, 2: 72%, 3: 90%)


###4.Summarize###

##build table combining all models##
Logistic_Error=c(0.16)
RandomForest_Error=c(0.20)
KNN_Error=c(0.19)
all_misclass=data.frame(Model,Logistic_Error,RandomForest_Error,KNN_Error)
library('xtable')
xtable(all_misclass)
