rm(list=ls())

###1.Preprocessing Data###

##import data##
library('readxl')
library('tidyr')
customer=read_excel('/Users/halabanz/Desktop/Applied Multivariate/customers.xlsx')
gen=read_excel('/Users/halabanz/Desktop/Applied Multivariate/generation.xlsx')
redvsblue=read_excel('/Users/halabanz/Desktop/Applied Multivariate/redvsblue.xlsx')

##clean customer data (2016)##
str(customer) #look at structure of dataset
customer2=customer[customer$Year==2016 & customer$`Industry Sector Category`=='Total Electric Industry',-c(3:8)] #select row for: 2016 entries and Total Electric Industry
sort(customer$State) #sort by alphabetical order, asc

##clean generation data (2016)##
str(gen)
gen2=gen[gen$YEAR==2016 & gen$`TYPE OF PRODUCER`=='Total Electric Power Industry',-c(3)] 
gen3=spread(gen2,`ENERGY SOURCE`,`GENERATION (Megawatthours)`) #reshape from long to wide with energy source as one column and gen in other
colnames(gen3)[5]='Hydroelectric'
sort(gen3$STATE)
summary(gen3)
gen4=gen3[,-c(4,10,12,14)] #remove columns not used in analysis

##merge data##
m1=merge(gen4,customer2,by.x = 'STATE',by.y = 'State') #merge generation and customer data
m2=m1[,-c(13)] #get rid of duplicate year
colnames(m2)[13]='Total_Customers' #rename column
m3=merge(m2,redvsblue,by.x='STATE',by.y = 'STATE') #merge m2 and data for political affilation
summary(m3)

##impute missing values to 0##
m3$Other[m3$Other<0]=0 #need get rid of negative values, will sqrt variables later
                       # only 3 negative values and they are small, so shouldnt impact analysis
m3[is.na(m3)] <- 0 #after doing some research, data seems to be missing because generation was at 0 or close to it
str(m3)
summary(m3)

##transform variables##
m4=m3 #making sure that data (m3) is available in its original form

#calulate total generation per customer#
m4$solar_gen_customer=(m4$`Solar Thermal and Photovoltaic`/m4$Total_Customers)
m4$coal_gen_customer=(m4$Coal/m4$Total_Customers)
m4$ng_gen_customer=(m4$`Natural Gas`/m4$Total_Customers)
m4$wind_gen_customer=(m4$Wind/m4$Total_Customers)
m4$bio_gen_customer=(m4$`Other Biomass`/m4$Total_Customers)
m4$hydro_gen_customer=(m4$Hydroelectric/m4$Total_Customers)
m4$wood_gen_customer=(m4$`Wood and Wood Derived Fuels`/m4$Total_Customers)
m4$petrol_gen_customer=(m4$Petroleum/m4$Total_Customers)
m4$nuclear_gen_customer=(m4$Nuclear/m4$Total_Customers)
m4$other_gen_customer=(m4$Other/m4$Total_Customers)

m5=m4[,15:24] #create subset of newly calculate covariates
m6=cbind(m4[,c(1,2,14)],m5) #append state, year, and affiliation to dataframe
colnames(m6)[c(4:13)]=c('solar','coal','ng','wind','bio','hydro','wood','petrol','nuclear','other') #change names

m6$Affiliation=as.factor(m6$Affiliation) #change affiliation from 2 to 4 to 0 to 1
levels(m6$Affiliation)=c('Red State','Blue State')
contrasts(m6$Affiliation)

str(m6) #check structure of final dataframe
m6[,4:13] <- lapply(m6[,4:13], as.numeric) #change from character to numeric


###2.Exploratory Analysis###

##2.1 bar charts of electricity generation by source##
library('ggplot2')
library('dplyr')
library('scales')

#calculate electricity generated by percentage of total generation#
percentage1 = mutate(m3, 
                     coal = (sum(m3$Coal) / sum(m3[,c(3:12)]))*100,
                     hydro = (sum(m3$Hydroelectric) / sum(m3[,c(3:12)]))*100,
                     bio = (sum(m3$`Other Biomass`) / sum(m3[,c(3:12)]))*100,
                     ng = (sum(m3$`Natural Gas`) / sum(m3[,c(3:12)]))*100,
                     nuclear = (sum(m3$Nuclear) / sum(m3[,c(3:12)]))*100,
                     nuclear = (sum(m3$Nuclear) / sum(m3[,c(3:12)]))*100,
                     petrol = (sum(m3$Petroleum) / sum(m3[,c(3:12)]))*100,
                     solar = (sum(m3$`Solar Thermal and Photovoltaic`) / sum(m3[,c(3:12)]))*100,
                     wind = (sum(m3$Wind) / sum(m3[,c(3:12)]))*100,
                     wood = (sum(m3$`Wood and Wood Derived Fuels`) / sum(m3[,c(3:12)]))*100,
                     other = (sum(m3$Other) / sum(m3[,c(3:12)]))*100)

percentage2=percentage1[1,c(15:24)]
percentage3=percentage2 %>% gather(Source, Total_Percent, coal:other) #reshape from wide to long
plot1=ggplot(data=percentage3,aes(x=Source,y=Total_Percent,fill=Source))+geom_bar(stat='identity')+xlab('Source')+ylab('% Total')+ggtitle('Electricity Generation in United States (2016)')+ylim(0,50)+  theme(plot.title = element_text(hjust = 0.5));plot1

##2.2 generate usa heatmap of electricity generation (Alaska and Hawaii included)##
install.packages("devtools")
devtools::install_github("wmurphyrd/fiftystater")
library(fiftystater)

data("fifty_states") # this line is optional due to lazy data loading
renewables <- data.frame(state = tolower(rownames(re)), re) #plug in re dataset here

#map_id creates the aesthetic mapping to the state name column in your data#
p <- ggplot(renewables, aes(map_id = State)) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = PercentRenewables), map = fifty_states) + 
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() +
  scale_x_continuous(breaks = NULL) + 
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom", 
        panel.background = element_blank());p


##2.3 boxplots##
m5[] <- lapply(m5,sqrt) #sqrt values in dataframe in order to make differences more clear in plot

#repeat steps from previous section#
m7=cbind(m4[,c(1,2,14)],m5) #append state, year, and affiliation to dataframe
colnames(m7)[c(4:13)]=c('solar','coal','ng','wind','bio','hydro','wood','petrol','nuclear','other') #change names
m7$Affiliation=as.factor(m7$Affiliation)
levels(m7$Affiliation)=c('Red State','Blue State')
contrasts(m7$Affiliation)
colnames(m7)[c(4:13)]=c('solar','coal','ng','wind','bio','hydro','wood','petrol','nuclear','other') #change names
m7[,4:13] <- lapply(m7[,4:13], as.numeric) #change from character to numeric
#plot#
aggregate(m6[,4:13],list(m6$Affiliation),mean) #numbers indicate large mean differences in coal, bio, and petrol
long_DF <- m7 %>% gather(Source, Generation, solar:other) #reshape from wide to long in order to plot
plot2=ggplot(data=long_DF,aes(x=Source,y=Generation,fill=Affiliation))+geom_boxplot()+xlab('Electricity Generation by Source')+ylab('Generation per Customer (âˆšMWH)')+ggtitle('Red States vs. Blue States (2016)')+  theme(plot.title = element_text(hjust = 0.5));plot2


###3.Building Models###

##3.1 Create training and test sets##
Train <- createDataPartition(m6$Affiliation, p=0.80, list=FALSE) 
m6_train <- m6[Train, ]
m6_test <- m6[ -Train, ]

##3.2 logistic regression (repeated 3 times to assess stability)##
install.packages("stargazer")
library('stargazer')
library('MASS')
library('car')
library('caret')

#first run full model#
logit1=glm(Affiliation ~ coal+wind+solar+bio+hydro+petrol+nuclear+wood+ng+other,family=binomial(link='logit'),data=m6_train);logit1
summary(logit1) #only coal and petrol statistically significant
logit1$fitted.values
vif1=vif(logit1);vif1 #all vif lower then 7, so no real issue with multicollinearity

#variable selection 1#
drop1(logit1,test = 'LRT') #keep coal bio ng
anova(logit1,test='LRT') #keep coal bio ng
drop1(logit1,test = 'Rao') #keep bio petrol
anova(logit1,test='Rao') #keep bio coal

#update model 1#
logit2=glm(Affiliation ~ coal+bio+ng+petrol,family=binomial(link='logit'),data = m6_train)
summary(logit2) #coal bio petrol statistically significant

#variable selection 2#
drop1(logit2,test = 'LRT') #keep coal bio 
anova(logit2,test='LRT') #keep coal bio 
drop1(logit2,test = 'Rao') #bio petrol
anova(logit2,test='Rao') #coal bio petrol

#update model 2, drop ng#
logit3=glm(Affiliation ~ coal+bio+petrol,family=binomial(link='logit'),data = m6_train)
summary(logit3) #bio and petrol are always significant at 5% level, coal at 10% level (less stable)

#variable selection 3#
drop1(logit3,test = 'LRT') #keep variables coal and bio
anova(logit3,test='LRT') #keep variables coal and bio
drop1(logit3,test = 'Rao') #bio petrol
anova(logit3,test='Rao') #keep all

#since no variables can be decisively dropped, logit3 model is final (coefficient estimates similar all 3 times)#
summary(logit3) #bio always significant at 5% level (stable)
                #petrol is usually significant at either 5% or 10% level (less stable)
                #coal is sometimes significant at the 10% level (less stable)

#prediction (average on all 3 training sets)#
p1 <- predict(logit3,newdata=m6_train,type='response')
p1 <- ifelse(p1 > 0.5,1,0)
p1=as.factor(p1)
str(p1)
levels(m6_train$Affiliation) <- c("0", "1")
misClasificError <- mean(p1 != m6_train$Affiliation);misClasificError
confusion <- table(p1,m6_train$Affiliation);confusion #in all 3 cases, false pos. > false neg.
print(paste('Accuracy',1-misClasificError)) #90% average accuracy (1: 90%, 2: 88%, 3: 92%)

#prediction (average on all 3 test sets)#
p2 <- predict(logit3,newdata=m6_test,type='response')
p2 <- ifelse(p2 > 0.5,1,0)
p2=as.factor(p2)
levels(m6_test$Affiliation) <- c("0", "1")
misClasificError <- mean(p2 != m6_test$Affiliation);misClasificError
confusion <- table(p2,m6_test$Affiliation);confusion #in all 3 cases, false pos. > false neg.
print(paste('Accuracy',1-misClasificError)) #84% average accuracy (1: 88%, 2: 88%, 3: 77%))

#Analyze quality of model#
install.packages('generalhoslem') #Hosmer-Lemeshow Stat
library('generalhoslem')
generalhoslem::logitgof(m6_train$Affiliation,fitted(logit3)) #p-value of 0.81 shows no evidence that predicted different from observed

install.packages('fmsb') #Neagelkerke
library('fmsb')
NagelkerkeR2(logit3) #R2 value of 0.72 indicates a model with decent prediction

OptimisedConc=function(model) #Concordance measure
{
  Data = cbind(model$y, model$fitted.values) 
  ones = Data[Data[,1] == 1,]
  zeros = Data[Data[,1] == 0,]
  conc=matrix(0, dim(zeros)[1], dim(ones)[1])
  disc=matrix(0, dim(zeros)[1], dim(ones)[1])
  ties=matrix(0, dim(zeros)[1], dim(ones)[1])
  for (j in 1:dim(zeros)[1])
  {
    for (i in 1:dim(ones)[1])
    {
      if (ones[i,2]>zeros[j,2])
      {conc[j,i]=1}
      else if (ones[i,2]<zeros[j,2])
      {disc[j,i]=1}
      else if (ones[i,2]==zeros[j,2])
      {ties[j,i]=1}
    }
  }
  Pairs=dim(zeros)[1]*dim(ones)[1]
  PercentConcordance=(sum(conc)/Pairs)*100
  PercentDiscordance=(sum(disc)/Pairs)*100
  PercentTied=(sum(ties)/Pairs)*100
  return(list("Percent Concordance"=PercentConcordance,"Percent Discordance"=PercentDiscordance,"Percent Tied"=PercentTied,"Pairs"=Pairs))
}
OptimisedConc(logit3) #model shows prediction of 95%, which is very good

#check for outliers#
N=length(m6_train$STATE) #global influence plot
STATE=1:N
hat.bw=hatvalues(logit3)
rstudent.bw=rstudent(logit3)
par(mfrow=c(2,2))
plot(hat.bw,rstudent.bw)
dffits.bw=dffits(logit3)
plot(STATE,dffits.bw,type = 'l')
cov.bw=covratio(logit3)
plot(STATE,cov.bw,type = 'l')
cook.bw=cooks.distance(logit3)
plot(STATE,cook.bw,type = 'l')
par(mfrow=c(1,1))
plot(STATE,cook.bw)
identify(STATE,cook.bw)
cook.bw

#observations 12 (HI), 14 (ID),and 33 (NM) look to be potential influential obs.
#Hawaii produces a lot of petrol for a blue state
#NM is a blue state that doesnt produce much biomass (37th) 
#ID produces the least amount of coal (21st) and most biomass (47th) per customer out of any red state

#create boxplots to compare outliers of ID and NM#
#coal for outliers compared to nations average#
plot3=ggplot(data = m7[33,],aes(x=STATE,y=coal))+geom_bar(stat='identity',fill='blue',width = 0.5);plot3
aggregate(m7$coal,list(m7$Affiliation),mean)
plot4=plot3+geom_bar(data=m7[14,],aes(x=STATE,y=coal),stat = 'identity',fill='red',width = 0.5)+geom_hline(data=m7,yintercept =  c(1.71),list(m7),color='blue')+xlab('States')+ylab('Coal per Customer (âˆšMWH)')+ggtitle('Comparison to Affiliaton Average: Coal (2016)')+theme(plot.title = element_text(hjust = 0.5));plot4
plot5=plot4+geom_hline(data=m7,yintercept =  c(3.981),list(m7),color='red');plot5

#biomass in idaho compared to nations average#
plot6=ggplot(data=m7[14,],aes(x=STATE,y=bio))+geom_bar(stat='identity',fill='red',width = 0.5);plot6
#bio for co and nm#
plot7=plot6+geom_bar(data=m7[33,],aes(x=STATE,y=bio),stat='identity',fill='blue',width = 0.5);plot7
#bio for outliers compared to nations average#
aggregate(m7$bio,list(m7$Affiliation),mean)
plot8=plot7+geom_hline(data=m7,yintercept = 0.19,color='red')+xlab('States')+ylab('Bio per Customer (âˆšMWH)')+ggtitle('Comparison to Affiliation Averages: Bio (2016)')+theme(plot.title = element_text(hjust = 0.5));plot7
plot9=plot8+geom_hline(data=m7,yintercept = 0.4286,color='blue');plot9

#new dataset with percentile ranks#
m8=m7[,c(1,3,5,8)]
percentile_df=mutate(m8,percent_rank_coal=ntile(m8$coal,100))
percentile_df=mutate(percentile_df,percent_rank_bio=ntile(percentile_df$bio,100))

#create tables for latex#
stargazer(logit3,title="Results", align=F)
model=factor(c('Reduced Model (bio+coal'))
misclassification_rate=c(misClasificError)
combine=data.frame(model,misclassification_rate)
library('xtable')
xtable(combine)


##3.3 random forest##
library('randomForest')
library('caret')

#build model#
par(mfrow=c(1,1))
rf1=randomForest(m6[,4:13],m6$Affiliation,mtry=3,data=m6,importance = TRUE,proximity = TRUE,ntree = 200) #first use default values for ntree and mtry
print(rf1) #oob error 19.61%, so 80% accuracy

#error rate of random forest
plot(rf1) #error stabilizes after 100 trees

#tune mtry
par(mfrow=c(1,1))
tuneRF(m6[,-c(1)],m6[,c(1)],stepFactor = 0.5,plot = TRUE,ntreeTry = 100,trace = TRUE,improve = 0.4) #anything above 3 mtry is the same meaning that all variables
varImpPlot(rf1) #bio and coal look to be only important variables


##3.4. KNN##
library(class)

#Generate a random number that is 80% of the total number of rows in dataset#
gen_rand <- sample(1:nrow(m6), 0.80 * nrow(m6)) 

#create normalization function#
normalization <-function(x) { (x -min(x))/(max(x)-min(x))} 
m6_norm <- as.data.frame(lapply(m6[,c(4:13)], normalization)) #run nomalization on all features
summary(m6_norm)

#create training/test#
m6_train <- m6_norm[gen_rand,] #create training
m6_test <- m6_norm[-gen_rand,] #create test 
m6_target_cat <- m6[gen_rand,3] #4th column is reponse, political affiliation
m6_test_cat <- m6[-gen_rand,3] #actual values

#run model (done 3 times to assess stability)#
knn1 <- knn(m6_train,m6_test,cl=m6_target_cat,k=3)  
confusion <- table(knn1,m6_test_cat) #confusion matrix
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(confusion) #81% average accuracy (1: 81%, 2: 72%, 3: 90%)


###4.Summarize###

##build table combining all models##
Logistic_Error=c(0.16)
RandomForest_Error=c(0.20)
KNN_Error=c(0.19)
all_misclass=data.frame(Model,Logistic_Error,RandomForest_Error,KNN_Error)
library('xtable')
xtable(all_misclass)
